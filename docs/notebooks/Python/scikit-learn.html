
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>scikit-learn - Linear regression &#8212; Programming Seminars</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="scikit-learn - Exercises" href="scikit_learn_exercises.html" />
    <link rel="prev" title="Setup And Libraries" href="SetupAndLibraries.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      <h1 class="site-logo" id="site-title">Programming Seminars</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Python Intro
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../PythonIntro/W1_Lecture.html">
   Variables and Decisions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../PythonIntro/exercises/W1_Exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../PythonIntro/W2_Lecture.html">
   Lists and Loops
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../PythonIntro/exercises/W2_Exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../PythonIntro/W3_Lecture.html">
   Data Processing and Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../PythonIntro/exercises/W3_Exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../PythonIntro/W4_Lecture.html">
   Plotting and Correlation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../PythonIntro/exercises/W4_Exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Python
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="SetupAndLibraries.html">
   Setup And Libraries
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   scikit-learn - Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="scikit_learn_exercises.html">
   scikit-learn - Exercises
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pandas.html">
   pandas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pandasDF.html">
   pandas - DataFrame Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linalg.html">
   Linalg and numpy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linalg_numpy_exercises.html">
   Linalg and numpy - Exercises
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tools
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Tools/Git.html">
   Git
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Tools/Latex.html">
   LaTeX
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  C++
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Cpp/Setup.html">
   Setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cpp/LectureNotes.html">
   C++ Intro Lecture Notes
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/Python/scikit-learn.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="scikit-learn-linear-regression">
<h1>scikit-learn - Linear regression<a class="headerlink" href="#scikit-learn-linear-regression" title="Permalink to this headline">¶</a></h1>
<p>We start with perhaps our simplest possible example, using <strong>Scikit-Learn</strong> to perform linear regression analysis on a data set produced by us.</p>
<p>What follows is a simple Python code where we have defined a function
<span class="math notranslate nohighlight">\(y\)</span> in terms of the variable <span class="math notranslate nohighlight">\(x\)</span>. Both are defined as vectors with  <span class="math notranslate nohighlight">\(100\)</span> entries.
The numbers in the vector <span class="math notranslate nohighlight">\(\hat{x}\)</span> are given
by random numbers generated with a uniform distribution with entries
<span class="math notranslate nohighlight">\(x_i \in [0,1]\)</span> (more about probability distribution functions
later). These values are then used to define a function <span class="math notranslate nohighlight">\(y(x)\)</span>
(tabulated again as a vector) with a linear dependence on <span class="math notranslate nohighlight">\(x\)</span> plus a
random noise added via the normal distribution.</p>
<p>The Numpy functions are imported used the <strong>import numpy as np</strong>
statement and the random number generator for the uniform distribution
is called using the function <strong>np.random.rand()</strong>, where we specificy
that we want <span class="math notranslate nohighlight">\(100\)</span> random variables.  Using Numpy we define
automatically an array with the specified number of elements, <span class="math notranslate nohighlight">\(100\)</span> in
our case.  With the Numpy function <strong>randn()</strong> we can compute random
numbers with the normal distribution (mean value <span class="math notranslate nohighlight">\(\mu\)</span> equal to zero and
variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> set to one) and produce the values of <span class="math notranslate nohighlight">\(y\)</span> assuming a linear
dependence as function of <span class="math notranslate nohighlight">\(x\)</span></p>
<div class="math notranslate nohighlight">
\[
y = 2x+N(0,1),
\]</div>
<p>where <span class="math notranslate nohighlight">\(N(0,1)\)</span> represents random numbers generated by the normal
distribution.  From <strong>Scikit-Learn</strong> we import then the
<strong>LinearRegression</strong> functionality and make a prediction <span class="math notranslate nohighlight">\(\tilde{y} =
\alpha + \beta x\)</span> using the function <strong>fit(x,y)</strong>. We call the set of
data <span class="math notranslate nohighlight">\((\hat{x},\hat{y})\)</span> for our training data. The Python package
<strong>scikit-learn</strong> has also a functionality which extracts the above
fitting parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> (see below). Later we will
distinguish between training data and test data.</p>
<p>For plotting we use the Python package
<a class="reference external" href="https://matplotlib.org/">matplotlib</a> which produces publication
quality figures. Feel free to explore the extensive
<a class="reference external" href="https://matplotlib.org/gallery/index.html">gallery</a> of examples. In
this example we plot our original values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> as well as the
prediction <strong>ypredict</strong> (<span class="math notranslate nohighlight">\(\tilde{y}\)</span>), which attempts at fitting our
data with a straight line.</p>
<p>The Python code follows here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing various packages</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">xnew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">ypredict</span> <span class="o">=</span> <span class="n">linreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xnew</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xnew</span><span class="p">,</span> <span class="n">ypredict</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="p">,</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Simple Linear Regression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/scikit-learn_4_0.png" src="../../_images/scikit-learn_4_0.png" />
</div>
</div>
<p>This example serves several aims. It allows us to demonstrate several
aspects of data analysis and later machine learning algorithms. The
immediate visualization shows that our linear fit is not
impressive. It goes through the data points, but there are many
outliers which are not reproduced by our linear regression.  We could
now play around with this small program and change for example the
factor in front of <span class="math notranslate nohighlight">\(x\)</span> and the normal distribution.  Try to change the
function <span class="math notranslate nohighlight">\(y\)</span> to</p>
<div class="math notranslate nohighlight">
\[
y = 10x+0.01 \times N(0,1),
\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is defined as before.  Does the fit look better? Indeed, by
reducing the role of the noise given by the normal distribution we see immediately that
our linear prediction seemingly reproduces better the training
set. However, this testing ‘by the eye’ is obviouly not satisfactory in the
long run. Here we have only defined the training data and our model, and
have not discussed a more rigorous approach to the <strong>cost</strong> function.</p>
<p>We need more rigorous criteria in defining whether we have succeeded or
not in modeling our training data.  You will be surprised to see that
many scientists seldomly venture beyond this ‘by the eye’ approach. A
standard approach for the <em>cost</em> function is the so-called <span class="math notranslate nohighlight">\(\chi^2\)</span>
function (a variant of the mean-squared error (MSE))</p>
<div class="math notranslate nohighlight">
\[
\chi^2 = \frac{1}{n}
\sum_{i=0}^{n-1}\frac{(y_i-\tilde{y}_i)^2}{\sigma_i^2},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> is the variance (to be defined later) of the entry
<span class="math notranslate nohighlight">\(y_i\)</span>.  We may not know the explicit value of <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>, it serves
however the aim of scaling the equations and make the cost function
dimensionless.</p>
<p>Minimizing the cost function is a central aspect of
our discussions to come. Finding its minima as function of the model
parameters (<span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> in our case) will be a recurring
theme in these series of lectures. Essentially all machine learning
algorithms we will discuss center around the minimization of the
chosen cost function. This depends in turn on our specific
model for describing the data, a typical situation in supervised
learning. Automatizing the search for the minima of the cost function is a
central ingredient in all algorithms. Typical methods which are
employed are various variants of <strong>gradient</strong> methods. These will be
discussed in more detail later. Again, you’ll be surprised to hear that
many practitioners minimize the above function ‘’by the eye’, popularly dubbed as
‘chi by the eye’. That is, change a parameter and see (visually and numerically) that
the  <span class="math notranslate nohighlight">\(\chi^2\)</span> function becomes smaller.</p>
<p>There are many ways to define the cost function. A simpler approach is to look at the relative difference between the training data and the predicted data, that is we define
the relative error (why would we prefer the MSE instead of the relative error?) as</p>
<div class="math notranslate nohighlight">
\[
\epsilon_{\mathrm{relative}}= \frac{\vert \hat{y} -\hat{\tilde{y}}\vert}{\vert \hat{y}\vert}.
\]</div>
<p>The squared cost function results in an arithmetic mean-unbiased
estimator, and the absolute-value cost function results in a
median-unbiased estimator (in the one-dimensional case, and a
geometric median-unbiased estimator for the multi-dimensional
case). The squared cost function has the disadvantage that it has the tendency
to be dominated by outliers.</p>
<p>We can modify easily the above Python code and plot the relative error instead</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">ypredict</span> <span class="o">=</span> <span class="n">linreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ypredict</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="s2">&quot;ro&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\epsilon_{\mathrm</span><span class="si">{relative}</span><span class="s1">}$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Relative error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/scikit-learn_12_0.png" src="../../_images/scikit-learn_12_0.png" />
</div>
</div>
<p>Depending on the parameter in front of the normal distribution, we may
have a small or larger relative error. Try to play around with
different training data sets and study (graphically) the value of the
relative error.</p>
<p>As mentioned above, <strong>Scikit-Learn</strong> has an impressive functionality.
We can for example extract the values of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> and
their error estimates, or the variance and standard deviation and many
other properties from the statistical data analysis.</p>
<p>Here we show an
example of the functionality of <strong>Scikit-Learn</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span> 
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">mean_squared_log_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">2.0</span><span class="o">+</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">ypredict</span> <span class="o">=</span> <span class="n">linreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The intercept alpha: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coefficient beta : </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="c1"># The mean squared error                               </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean squared error: </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ypredict</span><span class="p">))</span>
<span class="c1"># Explained variance score: 1 is perfect prediction                                 </span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Variance score: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ypredict</span><span class="p">))</span>
<span class="c1"># Mean squared log error                                                        </span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean squared log error: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ypredict</span><span class="p">)</span> <span class="p">)</span>
<span class="c1"># Mean absolute error                                                           </span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean absolute error: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ypredict</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ypredict</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="p">,</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Linear Regression fit &#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The intercept alpha: 
 [1.73564461]
Coefficient beta : 
 [[5.42387441]]
Mean squared error: 0.24
Variance score: 0.92
Mean squared log error: 0.01
Mean absolute error: 0.39
</pre></div>
</div>
<img alt="../../_images/scikit-learn_14_1.png" src="../../_images/scikit-learn_14_1.png" />
</div>
</div>
<p>The function <strong>coef</strong> gives us the parameter <span class="math notranslate nohighlight">\(\beta\)</span> of our fit while <strong>intercept</strong> yields
<span class="math notranslate nohighlight">\(\alpha\)</span>. Depending on the constant in front of the normal distribution, we get values near or far from <span class="math notranslate nohighlight">\(alpha =2\)</span> and <span class="math notranslate nohighlight">\(\beta =5\)</span>. Try to play around with different parameters in front of the normal distribution. The function <strong>meansquarederror</strong> gives us the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss defined as</p>
<div class="math notranslate nohighlight">
\[
MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2,
\]</div>
<p>The smaller the value, the better the fit. Ideally we would like to
have an MSE equal zero.  The attentive reader has probably recognized
this function as being similar to the <span class="math notranslate nohighlight">\(\chi^2\)</span> function defined above.</p>
<p>The <strong>r2score</strong> function computes <span class="math notranslate nohighlight">\(R^2\)</span>, the coefficient of
determination. It provides a measure of how well future samples are
likely to be predicted by the model. Best possible score is 1.0 and it
can be negative (because the model can be arbitrarily worse). A
constant model that always predicts the expected value of <span class="math notranslate nohighlight">\(\hat{y}\)</span>,
disregarding the input features, would get a <span class="math notranslate nohighlight">\(R^2\)</span> score of <span class="math notranslate nohighlight">\(0.0\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\tilde{\hat{y}}_i\)</span> is the predicted value of the <span class="math notranslate nohighlight">\(i-th\)</span> sample and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding true value, then the score <span class="math notranslate nohighlight">\(R^2\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
\]</div>
<p>where we have defined the mean value  of <span class="math notranslate nohighlight">\(\hat{y}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\]</div>
<p>Another quantity taht we will meet again in our discussions of regression analysis is
the mean absolute error (MAE), a risk metric corresponding to the expected value of the absolute error loss or what we call the <span class="math notranslate nohighlight">\(l1\)</span>-norm loss. In our discussion above we presented the relative error.
The MAE is defined as follows</p>
<div class="math notranslate nohighlight">
\[
\text{MAE}(\hat{y}, \hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1} \left| y_i - \tilde{y}_i \right|.
\]</div>
<p>We present the
squared logarithmic (quadratic) error</p>
<div class="math notranslate nohighlight">
\[
\text{MSLE}(\hat{y}, \hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n - 1} (\log_e (1 + y_i) - \log_e (1 + \tilde{y}_i) )^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\log_e (x)\)</span> stands for the natural logarithm of <span class="math notranslate nohighlight">\(x\)</span>. This error
estimate is best to use when targets having exponential growth, such
as population counts, average sales of a commodity over a span of
years etc.</p>
<p>Finally, another cost function is the Huber cost function used in robust regression.</p>
<p>The rationale behind this possible cost function is its reduced
sensitivity to outliers in the data set. In our discussions on
dimensionality reduction and normalization of data we will meet other
ways of dealing with outliers.</p>
<p>The Huber cost function is defined as</p>
<div class="math notranslate nohighlight">
\[
H_{\delta}(a)= {\begin{cases}{\frac {1}{2}}{a^{2}}&amp;{\text{for }}|a|\leq \delta , \delta (|a|-{\frac {1}{2}}\delta ),&amp;{\text{otherwise}}\end{cases}}
\]</div>
<p>Here <span class="math notranslate nohighlight">\(a=\boldsymbol{y} - \boldsymbol{\tilde{y}}\)</span>.
We will discuss in more
detail these and other functions in the various lectures.  We conclude this part with another example. Instead of
a linear <span class="math notranslate nohighlight">\(x\)</span>-dependence we study now a cubic polynomial and use the polynomial regression analysis tools of scikit-learn.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span><span class="mf">0.98</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)),</span><span class="mi">200</span><span class="p">))</span>
<span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="n">noise</span>
<span class="n">yn</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="mi">100</span>
<span class="n">poly3</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">poly3</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="n">clf3</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">clf3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">Xplot</span><span class="o">=</span><span class="n">poly3</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="n">poly3_plot</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">clf3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xplot</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cubic Fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">yn</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Cubic&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">y</span><span class="p">:</span>
        <span class="n">err</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yn</span><span class="p">)</span><span class="o">/</span><span class="n">yn</span>
    <span class="k">return</span> <span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">err</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">error</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/scikit-learn_28_0.png" src="../../_images/scikit-learn_28_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.005000000000000002
</pre></div>
</div>
</div>
</div>
<p>Let us now dive into  nuclear physics and remind ourselves briefly about some basic features about binding
energies.  A basic quantity which can be measured for the ground
states of nuclei is the atomic mass <span class="math notranslate nohighlight">\(M(N, Z)\)</span> of the neutral atom with
atomic mass number <span class="math notranslate nohighlight">\(A\)</span> and charge <span class="math notranslate nohighlight">\(Z\)</span>. The number of neutrons is <span class="math notranslate nohighlight">\(N\)</span>. There are indeed several sophisticated experiments worldwide which allow us to measure this quantity to high precision (parts per million even).</p>
<p>Atomic masses are usually tabulated in terms of the mass excess defined by</p>
<div class="math notranslate nohighlight">
\[
\Delta M(N, Z) =  M(N, Z) - uA,
\]</div>
<p>where <span class="math notranslate nohighlight">\(u\)</span> is the Atomic Mass Unit</p>
<div class="math notranslate nohighlight">
\[
u = M(^{12}\mathrm{C})/12 = 931.4940954(57) \hspace{0.1cm} \mathrm{MeV}/c^2.
\]</div>
<p>The nucleon masses are</p>
<div class="math notranslate nohighlight">
\[
m_p =  1.00727646693(9)u,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
m_n = 939.56536(8)\hspace{0.1cm} \mathrm{MeV}/c^2 = 1.0086649156(6)u.
\]</div>
<p>In the <a class="reference external" href="http://nuclearmasses.org/resources_folder/Wang_2017_Chinese_Phys_C_41_030003.pdf">2016 mass evaluation of by W.J.Huang, G.Audi, M.Wang, F.G.Kondev, S.Naimi and X.Xu</a>
there are data on masses and decays of 3437 nuclei.</p>
<p>The nuclear binding energy is defined as the energy required to break
up a given nucleus into its constituent parts of <span class="math notranslate nohighlight">\(N\)</span> neutrons and <span class="math notranslate nohighlight">\(Z\)</span>
protons. In terms of the atomic masses <span class="math notranslate nohighlight">\(M(N, Z)\)</span> the binding energy is
defined by</p>
<div class="math notranslate nohighlight">
\[
BE(N, Z) = ZM_H c^2 + Nm_n c^2 - M(N, Z)c^2 ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(M_H\)</span> is the mass of the hydrogen atom and <span class="math notranslate nohighlight">\(m_n\)</span> is the mass of the neutron.
In terms of the mass excess the binding energy is given by</p>
<div class="math notranslate nohighlight">
\[
BE(N, Z) = Z\Delta_H c^2 + N\Delta_n c^2 -\Delta(N, Z)c^2 ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta_H c^2 = 7.2890\)</span> MeV and <span class="math notranslate nohighlight">\(\Delta_n c^2 = 8.0713\)</span> MeV.</p>
<p>A popular and physically intuitive model which can be used to parametrize
the experimental binding energies as function of <span class="math notranslate nohighlight">\(A\)</span>, is the so-called
<strong>liquid drop model</strong>. The ansatz is based on the following expression</p>
<div class="math notranslate nohighlight">
\[
BE(N,Z) = a_1A-a_2A^{2/3}-a_3\frac{Z^2}{A^{1/3}}-a_4\frac{(N-Z)^2}{A},
\]</div>
<p>where <span class="math notranslate nohighlight">\(A\)</span> stands for the number of nucleons and the <span class="math notranslate nohighlight">\(a_i\)</span>s are parameters which are determined by a fit
to the experimental data.</p>
<p>To arrive at the above expression we have assumed that we can make the following assumptions:</p>
<ul class="simple">
<li><p>There is a volume term <span class="math notranslate nohighlight">\(a_1A\)</span> proportional with the number of nucleons (the energy is also an extensive quantity). When an assembly of nucleons of the same size is packed together into the smallest volume, each interior nucleon has a certain number of other nucleons in contact with it. This contribution is proportional to the volume.</p></li>
<li><p>There is a surface energy term <span class="math notranslate nohighlight">\(a_2A^{2/3}\)</span>. The assumption here is that a nucleon at the surface of a nucleus interacts with fewer other nucleons than one in the interior of the nucleus and hence its binding energy is less. This surface energy term takes that into account and is therefore negative and is proportional to the surface area.</p></li>
<li><p>There is a Coulomb energy term <span class="math notranslate nohighlight">\(a_3\frac{Z^2}{A^{1/3}}\)</span>. The electric repulsion between each pair of protons in a nucleus yields less binding.</p></li>
<li><p>There is an asymmetry term <span class="math notranslate nohighlight">\(a_4\frac{(N-Z)^2}{A}\)</span>. This term is associated with the Pauli exclusion principle and reflects the fact that the proton-neutron interaction is more attractive on the average than the neutron-neutron and proton-proton interactions.</p></li>
</ul>
<p>We could also add a so-called pairing term, which is a correction term that
arises from the tendency of proton pairs and neutron pairs to
occur. An even number of particles is more stable than an odd number.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/Python"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="SetupAndLibraries.html" title="previous page">Setup And Libraries</a>
    <a class='right-next' id="next-link" href="scikit_learn_exercises.html" title="next page">scikit-learn - Exercises</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Karl Henrik Fredly<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>